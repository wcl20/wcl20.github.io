---
layout: default
---
# Quality Diversity Through Surprise

## Overview
* Introduce three new quality diversity algorithms which employ surprise as a diversity measure, either on its own or combined with novelty

## Related Work

### Quality Diversity
* Quality diversity algorithms attempt to balance between their individuals' quality and their population's diversity
* The main purpose of quality diversity is the discovery of multiple and diverse good solutions at the same time. 
* The quality diversity paradigm has been inspired by the idea of solving single-objective optimization problems via multiobjective optimization

### Novelty Search
* Novelty faces a number of limitations as a measure of diversity
	- It lacks a temporal dimension in terms of the trends that evolution is following from one generation to the next.
* Novelty search rewards diversity by aggregating the entire evolutionary history into a novelty archive.
* Goldberg uses the notion of deception  to describe instances where highly-fit building blocks, when recombined, may guide search away from the global optimum.
* Divergent search tackles the problem directly by rewarding diversity at the phenotypical level, and a shift of paradigm is proposed where rewarding behavioral diversity becomes the predominant driver of evolution.
* Novelty Search rewards behaviors that have not been seen previously during search

### Novelty Search with Local Competition
* Novelty search with local competition is an algorithm that combines the divergence of novelty search with the localized convergence obtained through a local competition
* Uses multi-objective algorithm NSGA-II to find solutions across two dimensions: novelty and local competition. 

## Content
* The new algorithms are inspired by Novelty Search with Local Competition:
	- Replaces Novelty Search with Surprise Search SSLC
	- Combines measures Surprise and Novelty Linearly NSSLC
	- Uses Novelty and Surprise as separate objectives NS-SS-LC
* The core contribution of this paper is in the introduction of surprise as a mechanism for quality diversity in the form of three different algorithms.
* Overview of the algorithms:
	1. Generate a population and initialize novelty archive
	2. Evaluate each individual according to algorithm
	3. Two parents are selected to generate an offspring, the offspring is evaluated according to algorithm
	4. The worst individual of the population is then replaced by the newly generated offspring if offspring is more fit.
	5. Update surprise model every N offspring
	6. Repeat 3-5 until termination condition

### Surprise Search
* In the context of Evolution Computation (EC) surprise is more broadly defined as deviation from expected behaviors.
* surprise search considers the recent historical trends to make predictions about the future and deviate from them.
* Surprise accounts for behavioral trends in recent generations and uses them to predict future behaviors.
* Rewards unexpected behaviors based on past behavioral trends observed during search.
* Surprise search requires two components: 
	- prediction model: uses past behaviors to estimate future behaviors. 
	- deviation model: rewards each individual in the current population based on its distance from the closest prediction
* surprise search allows the algorithm to backtrack, re-visiting areas of the search space that have been explored in previous generations.

### Surprise Search with Local Competiton
* Replaces Novelty with Surprise is NSLC

### Novelty-Surprise Search with Local Competition
* NSGA-II searches for non-dominated solutions on the dimensions of local competition and a weighted sum combining novelty and surprise.

### Novelty Search and Surprise Search with Local Competition
* The three-objective algorithm uses NSGA-II to search for non-dominated solutions on the three dimensions: local competition score, surprise score, novelty score

## Experiment
* One of the most popular testbeds for divergent search and quality diversity algorithms is the maze navigation problem
* Deceptive mazes are easy to identify visually, as they feature dead ends along the direct line between starting and goal position.
* As an indication of how deceptive landscapes can hinder traditional EC approaches, objective search is used as a baseline.
* In the maze navigation task, local competition counts the number of neighboring robots with final positions that are farther from the goal in terms of Euclidean distance.
* In this task, the surprise model predicts the robots final position.
* NSS-LC and NS-LC perform much better compared to SS-LC
* NSS-LC outperforms the other QD approaches considered in this work, namely NS-LC, NS-SSLC and SS-LC
* NSS-LC requires significantly fewer evaluations than every other algorithm
* NS-SS-LC has a very similar performance as NS-LC, while  SS-LC underperforms by comparison.
* NSS-LC performs better at higher $\lambda$ values (0.6, 0.7) indicates that the novelty score primarily drives the search and surprise is favored in specific circumstances.

## Future Work
* Surprise could be integrated into MAP-Elites, for instance, by storing two different maps: one used in the normal algorithm and the other used for prediction, with deviation from predictions used as a selection process for the algorithm.
* Exploring different prediction models for the computation of surprise.




