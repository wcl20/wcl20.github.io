---
layout: default
---
# Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning

## Overview
* Presents a hierarchical learning framework that improves sample-efficiency and generalizability of locomotion skills on real-world robots.

## Related Work

### Model-free Reinforcement Learning
* Model-free RL optimizes a policy $\pi$ by directly maximizing the long-term reward, without reasoning about the dynamics.
* Model-free methods are typically less sample-efficient than model-based but achieve better asymptotic performance. 
* Soft Actor Critic is an off-policy, model-free learning algorithm that maximizes both the long-term reward and the entropy of the policy.

### Model Predictive Control
* MPC utilizes the dynamics to solve for an action sequence that maximizes the long-term reward.

### Hierarchical Reinforcement Learning
* Using a hierarchical structure that decomposes complex task control into easier sub-tasks control can speed up learning.
* Divides the task into learning primitives, followed by planning in primitive space.

## Content
* We address two of the main challenges facing learning for legged locomotion research: 
	- sample efficiency and 
	- generalization
* In this work, we improve sample-efficiency of RL on locomotion by using a cyclic parametrization of walking policies.
* We improve generalization to multiple goals by proposing an efficient hierarchical structure.

### Hierarchical Learning Framework
* We divide the problem of goal-oriented locomotion into two sub-problems:
	- Learning diverse primitive skills such as turning and walking straight using model-free RL
	- Using model-based learning to sequence these skills
* Our main contribution is a hierarchical framework that combines model-free learning with model-based planning to improves generalization of locomotion skills to new goals. By incorporating dynamics models in Hierarchical RL, we can improve sample-efficiency as well as generalization.
* Our approach builds a library of primitives that encode low-level controllers for 4 micro-actions turn left, turn right, move forward, stand still.
* We take inspiration from biological gaits in locomotion and use two cyclic parametrizations for our action primitives.
* Action primitives take as input a phase variable $t \in (0,1]$ and outputs the next desired joint configuration as an action. At the beginning of every cycle, the phase variable is initialized to 0, and then grows linearly to 1.
* Parameters of actions primitives is learnt using SAC algorithm. Two types of parameterization:
	- Neural Network Policy: input phase variable $t$, outputs 18 desired joint angles
	- Sinusoidal Policy: For each motor, angle at phase $t$ is given by $A sin(2\pi t + B) + C$. Learn $A,B,C$ for each motor thus total of 54 parameters.
* We use a model-based high-level planner that plans the best primitive sequence using MPC.

## Experiment

### Daisy Hexapod
* Daisy is a six-legged robot with three motors on each leg. A Vive tracking system is used to measure robot's position in the global frame.
* We train two primitive actions on Daisy : walk forward, and turn right.
* In simulation, for the forward task, the neural network learns faster than the sinusoidal controller, and the reward is also higher than the sinusoidal controller. 
* Although in simulation the neural network trains faster than the sinusoidal controller, we were not successful in training a neural network policy from scratch on hardware, possibly due to noise in reward generation.
* Tested two experimental settings (simulation and hardware):
	- Different goals: Robot has to move to different goals starting at (0,0)
	- Waypoint goals: Robot has to achieve waypoints sequentially starting at (0,0)
* In simulation, we compare the high-level control against PETS. 
	- HRL is able to reach targets. 
	- PETS only reached the goal that the dynamics were learned on efficiently, it does not generalize to other goals in the environment.
* Our hardware experiments show that our proposed hierarchical controller can achieve far away goals using very small amount of training data on hardware.

## Future Work
* Waypoints can be generated by a separate controller that takes the environment state as input, for example with an image.
* One could try to incrementally learn new primitives for achieving new targets, and store them in the library for future reusability.




